{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **JEDNODIMENZIONE NUMERIČKE METODE**\n",
    "### **GRADIJENTNE METODE**\n",
    "Uslov je da je funkcija diferencijabilna do reda koji je potreban:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "  f = x ** 2 - np.sin(2 * x)\n",
    "  return f\n",
    "\n",
    "def dfunc(x):\n",
    "  f = 2 * x - 2 * np.cos(2 * x)\n",
    "  return f\n",
    "\n",
    "def ddfunc(x):\n",
    "  f = 2 + 4 * np.sin(2 * x)\n",
    "  return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Njutn-Rapsonov metod**\n",
    "\n",
    "   $$ x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} $$\n",
    "   - **Opis**: Njutn-Rapsonov metod koristi prve i druge izvode funkcije kako bi iterativno pronašao tačku minimuma ili maksimuma funkcije.\n",
    "   - **Kako radi**: Algoritam započinje sa početnom tačkom $ x_0 $. Zatim koristi tangentu u toj tački i nalazi tačku preseka te tangente sa x-osom kako bi predvideo novu tačku koja je bliža rešenju. Taj proces se ponavlja dok ne dođemo dovoljno blizu rešenja.\n",
    "   - **Prednosti**:\n",
    "     - **Brza konvergencija**: Ako je početna tačka blizu rešenja i funkcija je konveksna, algoritam može brzo da konvergira do optimalnog rešenja.\n",
    "     - **Tačnost**: Za funkcije sa jasnim tačkama minimuma/maksimuma, može biti veoma precizan.\n",
    "   - **Mane**:\n",
    "     - **Osetljivost na početnu tačku**: Ako početna tačka nije blizu rešenja ili funkcija ima više ekstrema, može konvergirati ka pogrešnoj tački.\n",
    "     - **Nije univerzalan**: Pogodan je samo za funkcije koje su dvaput diferencijabilne.\n",
    "   - **Korišćenje**: Najčešće se koristi za funkcije koje su glatke (diferencijabilne) i kada postoji potreba za brzim pronalaženjem optimizovanih vrednosti.\n",
    "   - **Kriterijum zaustavljanja**: Iteracija se zaustavlja kada razlika između uzastopnih iteracija bude manja od zadatog praga $ \\epsilon $ : $$ |x_n - x_{n-1}| < \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson_metod(x, epsilon):\n",
    "    x1 = x\n",
    "    x = math.inf\n",
    "    while abs(x1 - x) > epsilon:\n",
    "        x = x1\n",
    "        x1 = x - dfunc(x) / ddfunc(x)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Metod sečice**\n",
    "\n",
    "   - **Formula**: $$x_{n+1} = x_n - f'(x_n) \\frac{x_n - x_{n-1}}{f'(x_n) - f'(x_{n-1})} $$\n",
    "   - **Opis**: Metod sečice koristi aproksimaciju drugog izvoda i koristi dve tačke kako bi pronašao bolju iterativnu tačku.\n",
    "   - **Kako radi**: Počinje sa dve tačke $x_{n-1} $ i $x_n $, a zatim koristi te dve tačke da aproksimira tangentnu liniju kroz njih. Novu tačku dobija presekom tangentne linije sa x-osom.\n",
    "   - **Prednosti**:\n",
    "     - **Manja zavisnost od početne tačke**: Metod sečice može biti stabilniji od Njutn-Rapsonovog metoda jer koristi dve početne tačke.\n",
    "   - **Mane**:\n",
    "     - **Sporija konvergencija**: Generalno, metod sečice može da bude sporiji od Njutn-Rapsonovog metoda.\n",
    "   - **Korišćenje**: Koristi se kada se ne može lako izračunati drugi izvod funkcije.\n",
    "   - **Kriterijum zaustavljanja**: Slično kao kod Njutn-Rapsonovog metoda, zaustavlja se kada je razlika između uzastopnih iteracija manja od praga $\\epsilon $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metod_secice(x0, x, epsilon):\n",
    "    x1 = x\n",
    "    x = x0\n",
    "    while abs(x1 - x) > epsilon:\n",
    "        x0 = x\n",
    "        x = x1\n",
    "        x1 = x - dfunc(x) * (x - x0) / (dfunc(x) - dfunc(x0))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **METODE DIREKTNOG PRETRAŽIVANJA**\n",
    "\n",
    "Uslov je da je funkcija unimodalna.\n",
    "\n",
    "\n",
    "#### 3. **Fibonačijev metod**\n",
    "\n",
    "   - **Formula**: $$x_1 = a + \\frac{F_{N-2}}{F_N} (b - a) \\qquad \\qquad \\qquad x_2 = a + b - x_1 $$\n",
    "   - **Opis**: Koristi Fibonačijeve brojeve kako bi smanjio interval pretrage na efikasan način i pronašao minimum unutar intervala.\n",
    "   - **Kako radi**: Metod deli interval tako da novo područje istraživanja predstavlja deo zlatnog preseka koristeći omere Fibonačijevih brojeva.\n",
    "   - **Prednosti**:\n",
    "     - **Efikasnost kod unimodalnih funkcija**: Za funkcije koje imaju samo jedan minimum u zadatom intervalu, ovaj metod može brzo da pronađe rešenje.\n",
    "   - **Mane**:\n",
    "     - **Računska složenost**: Potrebno je pratiti niz Fibonačijevih brojeva, što može biti složenije za implementaciju.\n",
    "   - **Korišćenje**: Koristi se kada je poznato da funkcija ima jedinstven minimum unutar intervala.\n",
    "   - **Kriterijum zaustavljanja**: Proces se zaustavlja kada zadovolji uslov: $$F_N > \\frac{b - a}{\\epsilon} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {0: 1, 1: 1, 2: 1}\n",
    "\n",
    "def fib(n):\n",
    "    if n in cache:\n",
    "        return cache[n]\n",
    "    else:\n",
    "        cache[n] = fib(n - 1) + fib(n - 2)\n",
    "        return cache[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako se traži minimum funkcije, $x$ koje ima manje $f(x)$ će postati novo $a$, odnosno $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacijev_metod(a, b, epsilon):\n",
    "    n = 1\n",
    "\n",
    "    while fib(n) < (b - a) / epsilon:\n",
    "        n += 1\n",
    "\n",
    "    x1 = a + fib(n-2) / fib(n) * (b - a)\n",
    "    x2 = a + b - x1\n",
    "\n",
    "    for _ in range(1, n):\n",
    "        if func(x1) < func(x2):\n",
    "            b = x2\n",
    "        else:\n",
    "            a = x1\n",
    "        x1 = a + fib(n-2) / fib(n) * (b - a)\n",
    "        x2 = a + b - x1\n",
    "\n",
    "    if func(x1) < func(x2):\n",
    "        return x1\n",
    "    else:\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Metod zlatnog preseka**\n",
    "\n",
    "   - **Formula**: $$x_1 = a + c (b - a) \\qquad \\qquad \\qquad x_2 = a + b - x_1 $$ gde je $c = \\frac{3 - \\sqrt{5}}{2} $\n",
    "   \n",
    "   \n",
    "   - **Opis**: Ovo je još jedan metod pretrage intervala za pronalaženje minimuma, baziran na zlatnom preseku.\n",
    "   - **Kako radi**: Metod deli interval na dva dela korišćenjem zlatnog preseka, kako bi se istražio deo koji najverovatnije sadrži minimum.\n",
    "   - **Prednosti**:\n",
    "     - **Jednostavnost i stabilnost**: Dobro radi za unimodalne funkcije i ne zahteva računanje izvoda.\n",
    "   - **Mane**:\n",
    "     - **Ograničenja u višedimenzionalnom prostoru**: Metod je najefikasniji za jednodimenzionalne probleme.\n",
    "   - **Korišćenje**: Primenjuje se kod minimizacije funkcija koje su unimodalne na zadatom intervalu.\n",
    "   - **Kriterijum zaustavljanja**: Iteracija se prekida kada je dužina intervala manja od praga $\\epsilon$: $$b - a < \\epsilon $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metod_zlatnog_preseka(a, b, epsilon):\n",
    "    c = (3 - math.sqrt(5)) / 2\n",
    "    x1 = a + c * (b - a)\n",
    "    x2 = a + b - x1\n",
    "\n",
    "    while b - a > epsilon:\n",
    "        if func(x1) < func(x2):\n",
    "            b = x2\n",
    "        else:\n",
    "            a = x1\n",
    "        x1 = a + c * (b - a)\n",
    "        x2 = a + b - x1\n",
    "\n",
    "    if func(x1) < func(x2):\n",
    "        return x1\n",
    "    else:\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "#### **Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5149438521495289\n",
      "0.514926765787982\n",
      "0.5154524653206354\n",
      "0.515441564939853\n"
     ]
    }
   ],
   "source": [
    "print(newton_raphson_metod(x=-1, epsilon=0.01))\n",
    "print(metod_secice(x0=-1, x=1, epsilon=0.01))\n",
    "print(fibonacijev_metod(a=-1, b=1, epsilon=0.01))\n",
    "print(metod_zlatnog_preseka(a=-1, b=1, epsilon=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VIŠEDIMENZIONE NUMERIČKE METODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "def grad(x):\n",
    "    x = np.array(x).reshape(np.size(x))\n",
    "    return np.asarray([[2 * x[0]], [2 * x[1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "#### 5. **Metod najbržeg pada (Gradient Descent)**\n",
    "\n",
    "   - **Formula**: $$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma \\nabla f(\\mathbf{x}_k) $$\n",
    "   - **Opis**: Metod najbržeg pada koristi gradijent funkcije za traženje pravca u kojem funkcija opada najbrže.\n",
    "   - **Kako radi**: Počinje sa nasumičnom tačkom $\\mathbf{x}_0 $ i zatim se iterativno pomera niz gradijent funkcije koristeći korak veličine $\\gamma $\n",
    "   - **Prednosti**:\n",
    "     - **Jednostavan za implementaciju** i široko primenljiv.\n",
    "   - **Mane**:\n",
    "     - **Može zapeti u lokalnim minimumima** i zahteva podešavanje parametra $\\gamma $\n",
    "   - **Korišćenje**: Koristi se u optimizaciji parametara u mašinskom učenju i drugim konveksnim problemima.\n",
    "   - **Kriterijum zaustavljanja**: Iteracija se zaustavlja kada je norma gradijenta manja od $\\epsilon $: $$\\|\\nabla f(\\mathbf{x}_k)\\| \\leq \\epsilon $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krećemo se niz gradijent promenljivom gama, jer tražimo minimum. On može da upadne u prvi lokalni minimum pa da ne nađe globalni. Kriterijum zaustavljanja je ?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metod_najbrzeg_pada(x, gamma, epsilon, N):\n",
    "    x = np.array(x).reshape(len(x), 1)\n",
    "    for _ in range(N):\n",
    "        g = grad(x)\n",
    "        x = x - gamma * g\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Metod najbržeg pada sa momentom**\n",
    "\n",
    "   - **Formula**: $$\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}_k) \\qquad \\qquad \\qquad \\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k $$\n",
    "   - **Opis**: Dodaje momenat (smer prethodnog gradijenta) u izračunavanje kako bi se smanjile oscilacije ($ \\mathbf{v} $ je vektor brzine koji sadrži informaciju o prethodnom pravcu kretanja, dok $ \\omega $ predstavlja koeficijent momenta (tipično između 0 i 1) koji određuje koliko \"pamtimo\" prethodni pravac) \n",
    "   - **Kako radi**: Čuva pravac prethodnog koraka i koristi ga da bi stabilizovao putanju u pravcu minimuma.\n",
    "   - **Prednosti**:\n",
    "     - **Stabilnija konvergencija** i smanjenje oscilacija u blizini minimuma.\n",
    "   - **Mane**:\n",
    "     - **Podešavanje parametara** može biti kompleksno.\n",
    "   - **Korišćenje**: Koristi se u treniranju modela gde osnovni metod najbržeg pada pokazuje oscilacije.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metod_najbrzeg_pada_sa_momentom(x, gamma, epsilon, omega, N):\n",
    "    x = np.array(x).reshape(len(x), 1)\n",
    "    v = 0\n",
    "    for _ in range(N):\n",
    "        g = grad(x)\n",
    "        v = omega * v + gamma * g\n",
    "        x = x - v\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ADAPTIVNI GRADIJENTNI METODI**\n",
    "\n",
    "Intenzitet promene u određenoj osi je specifičan za tu osu.\n",
    "\n",
    "\n",
    "#### 7. **Adagrad (Adaptivni gradijent)**\n",
    "\n",
    "   - **Formula**: $$x_{k+1,i} = x_{k,i} - \\frac{\\gamma}{\\sqrt{G_{k,i} + \\epsilon_1}} g_{k,i} $$\n",
    "   - **Opis**: Adaptivno prilagođava učestalost učenja za svaku dimenziju posebno.\n",
    "\n",
    "**$ \\qquad \\qquad g $** je gradijent funkcije u trenutnoj iteraciji, tj. izračunati smer promena za svaku promenljivu.\n",
    "\n",
    "\n",
    "**$ \\qquad \\qquad G $** je suma kvadrata svih prethodnih gradijenata po svakoj dimenziji\n",
    "\n",
    "\n",
    "**$ \\qquad \\qquad \\epsilon_1 $** je mali broj (poput $ 10^{-8} $) dodat da bi se izbegla deljenje sa nulom.\n",
    "\n",
    "\n",
    "**$ \\qquad \\qquad \\gamma $** je početna učestalost učenja koja se skalira za svaku promenljivu u zavisnosti od vrednosti $ G $.\n",
    "\n",
    "\n",
    "   - **Kako radi**: Računa sumu kvadrata prethodnih gradijenata kako bi prilagodio korak u svakoj iteraciji.\n",
    "   - **Prednosti**:\n",
    "     - **Automatska prilagodljivost** učestalosti učenja.\n",
    "   - **Mane**:\n",
    "     - **Smanjenje učestalosti učenja** može biti prebrzo, što usporava konvergenciju.\n",
    "   - **Korišćenje**: Pogodan za probleme sa velikim brojem parametara, kao što su duboke neuronske mreže.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(x, gamma, epsilon, epsilon1, N):\n",
    "    x = np.array(x).reshape(len(x), 1)\n",
    "    v = 0\n",
    "    G = 0\n",
    "    for _ in range(N):\n",
    "        g = grad(x)\n",
    "        G = G + np.multiply(g, g)\n",
    "        v = gamma * g / np.sqrt(G + epsilon1)\n",
    "        x = x - v\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ADAM**\n",
    "\n",
    "Pomoćne veličine: $$m_k = \\omega_1 m_{k-1} + (1-\\omega_1) g_k$$\n",
    "$$v_k = \\omega_2 v_{k-1} + (1-\\omega_2) g_k^2$$\n",
    "$$\\hat m_k = \\frac{m_k}{1-\\omega_1}$$\n",
    "$$\\hat v_k = \\frac{v_k}{1-\\omega_2}$$\n",
    "\n",
    "\n",
    "   - **Formula**:\n",
    "     $$x_{k+1} = x_k - \\frac{\\gamma}{\\sqrt{\\hat v_k + \\epsilon_1}} \\hat m_k $$\n",
    "\n",
    "\n",
    " **$ \\qquad \\qquad g $** je gradijent funkcije u trenutnoj iteraciji, koji predstavlja smer u kojem se optimizacija vrši za svaku promenljivu.\n",
    "\n",
    "\n",
    " **$ \\qquad \\qquad m $** je **prosečni gradijent** (prvi moment) do trenutne iteracije, što pomaže da se uhvati dugoročni smer kretanja u optimizaciji.\n",
    "\n",
    "\n",
    " **$ \\qquad \\qquad v $** je **prosečan kvadrat gradijenta** (drugi moment), koji pomaže da se kontroliše veličina koraka i ublaže nagle promene u gradijentima.\n",
    "\n",
    "\n",
    " **$ \\qquad \\qquad \\omega_1 $** i **$ \\omega_2 $** su koeficijenti za eksponencijalno izravnjavanje prvog i drugog momenta (obično vrednosti 0,9 i 0,999).\n",
    "\n",
    "\n",
    " **$ \\qquad \\qquad\\epsilon_1 $** je mali broj dodat da spreči deljenje sa nulom (poput $ 10^{-8} $).\n",
    "\n",
    " \n",
    " **$ \\qquad\\qquad\\gamma $** je početna učestalost učenja, skalirana sa $ v $ kako bi koraci učenja bili stabilniji i prilagodljiviji različitim promenljivim.\n",
    "\n",
    "\n",
    "   - **Opis**: Kombinuje prednosti Adagrad i RMSProp optimizatora kako bi stabilizovao učenje.\n",
    "   - **Kako radi**: Koristi dva momenta (prvi momenat za prosečan gradijent, drugi za kvadratni gradijent).\n",
    "   - **Prednosti**:\n",
    "     - **Stabilna i efikasna konvergencija**, pogodan za probleme sa nelinearnim funkcijama.\n",
    "   - **Mane**:\n",
    "     - **Veći broj hiperparametara**, što može otežati podešavanje.\n",
    "   - **Korišćenje**: Najčešće se koristi za treniranje dubokih neuronskih mreža.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x, gamma, omega1, omega2, epsilon, epsilon1, N):\n",
    "    x = np.array(x).reshape(len(x), 1)\n",
    "    v = 1\n",
    "    m = 1\n",
    "    for _ in range(N):\n",
    "        g = grad(x)\n",
    "        m = omega1 * m + (1 - omega1) * g\n",
    "        v = omega2 * v + (1 - omega2) * np.multiply(g, g)\n",
    "        hat_v = np.abs(v / (1 - omega2))\n",
    "        hat_m = m / (1 - omega1)\n",
    "        x = x - gamma * hat_m / np.sqrt(hat_v + epsilon1)\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "#### **Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00241785]\n",
      " [ 0.00241785]]\n",
      "[[-0.0025769]\n",
      " [ 0.0025769]]\n",
      "[[-0.00275321]\n",
      " [ 0.00275321]]\n",
      "[[-0.00217151]\n",
      " [ 0.00200796]]\n"
     ]
    }
   ],
   "source": [
    "print(metod_najbrzeg_pada(x=[-1, 1], gamma=0.1, epsilon=0.01, N=100))\n",
    "print(metod_najbrzeg_pada_sa_momentom(x=[-1, 1], gamma=0.1, epsilon=0.01, omega=0.1, N=100))\n",
    "print(adagrad(x=[-1, 1], gamma=0.3, epsilon=0.01, epsilon1=1e-8, N=100))\n",
    "print(adam(x=[-1, 1], gamma=0.1, omega1=0.1, omega2=0.9, epsilon=0.01, epsilon1=1e-8, N=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FITOVANJE KRIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1.9 * x + 0.48\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2.5, 4, 6.5, 8.2, 10]\n",
    "\n",
    "p = np.polyfit(x, y, 1)\n",
    "\n",
    "# plt.plot(x, y)\n",
    "# plt.plot(x, np.polyval(p, x))\n",
    "# plt.show()\n",
    "\n",
    "print(f\"y = {p[0]:.2} * x + {p[1]:.2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
